{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, XLNetTokenizer, XLNetModel, XLNetLMHeadModel, XLNetConfig\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n",
      "Number of GPU Available: 16\n",
      "GPU: Tesla K80\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU Available: {}\".format(torch.cuda.is_available()))\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(\"Number of GPU Available: {}\".format(n_gpu))\n",
    "print(\"GPU: {}\".format(torch.cuda.get_device_name(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true = pd.read_csv('True.csv')\n",
    "df_fake = pd.read_csv('Fake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true['label'] = 0\n",
    "df_fake['label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21417, 5) (23481, 5)\n"
     ]
    }
   ],
   "source": [
    "print(df_true.shape,df_fake.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_colwidth', -1)\n",
    "# df_true[['title','text']].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.concat([df_true,df_fake],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'text', 'subject', 'date', 'label'], dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df_full[['title', 'text','label']]\n",
    "df_full.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full['fulltext'] = df_full['title'] + ' ' + df_full['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = train_test_split(df_full,test_size = 0.5,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22449, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22449, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_list = train['fulltext'].values\n",
    "test_text_list = test['fulltext'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sentence_embeddings_length(text_list, tokenizer):\n",
    "    tokenized_texts = list(map(lambda t: tokenizer.tokenize(t), text_list))\n",
    "    tokenized_texts_len = list(map(lambda t: len(t), tokenized_texts))\n",
    "    fig, ax = plt.subplots(figsize=(8, 5));\n",
    "    ax.hist(tokenized_texts_len, bins=40);\n",
    "    ax.set_xlabel(\"Length of Comment Embeddings\");\n",
    "    ax.set_ylabel(\"Number of Comments\");\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_sentence_embeddings_length(train_text_list, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_sentence_embeddings_length(test_text_list, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeddings=126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_inputs(text_list, tokenizer, num_embeddings=num_embeddings):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text input into ids. Appends the appropriate special\n",
    "    characters to the end of the text to denote end of sentence. Truncate or pad\n",
    "    the appropriate sequence length.\n",
    "    \"\"\"\n",
    "    # tokenize the text, then truncate sequence to the desired length minus 2 for\n",
    "    # the 2 special characters\n",
    "    tokenized_texts = list(map(lambda t: tokenizer.tokenize(t)[:num_embeddings-2], text_list))\n",
    "    # convert tokenized text into numeric ids for the appropriate LM\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "    # append special token \"<s>\" and </s> to end of sentence\n",
    "    input_ids = [tokenizer.build_inputs_with_special_tokens(x) for x in input_ids]\n",
    "    # pad sequences\n",
    "    input_ids = pad_sequences(input_ids, maxlen=num_embeddings, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    return input_ids\n",
    "\n",
    "def create_attn_masks(input_ids):\n",
    "    \"\"\"\n",
    "    Create attention masks to tell model whether attention should be applied to\n",
    "    the input id tokens. Do not want to perform attention on padding tokens.\n",
    "    \"\"\"\n",
    "    # Create attention masks\n",
    "    attention_masks = []\n",
    "\n",
    "    # Create a mask of 1s for each token followed by 0s for padding\n",
    "    for seq in input_ids:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "    return attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22449"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids = tokenize_inputs(train_text_list, tokenizer, num_embeddings=num_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids = tokenize_inputs(test_text_list, tokenizer, num_embeddings=num_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_attention_masks = create_attn_masks(train_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_attention_masks = create_attn_masks(test_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22449, 126)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.copy()\n",
    "test = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add input ids and attention masks to the dataframe\n",
    "train[\"features\"] = train_input_ids.tolist()\n",
    "train[\"masks\"] = train_attention_masks\n",
    "\n",
    "test[\"features\"] = test_input_ids.tolist()\n",
    "test[\"masks\"] = test_attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = train_test_split(train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[\"features\"].values.tolist()\n",
    "X_valid = valid[\"features\"].values.tolist()\n",
    "\n",
    "train_masks = train[\"masks\"].values.tolist()\n",
    "valid_masks = valid[\"masks\"].values.tolist()\n",
    "\n",
    "Y_train = [[0,1] if label==1 else [1,0]\\\n",
    "           for label in list(train['label']) ]\n",
    "Y_valid = [[0,1] if label==1 else [1,0]\\\n",
    "           for label in list(valid['label']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train)\n",
    "X_valid = torch.tensor(X_valid)\n",
    "\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.float32)\n",
    "Y_valid = torch.tensor(Y_valid, dtype=torch.float32)\n",
    "\n",
    "train_masks = torch.tensor(train_masks, dtype=torch.long)\n",
    "valid_masks = torch.tensor(valid_masks, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training\n",
    "batch_size = 16\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on \n",
    "# memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(X_train, train_masks, Y_train)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data,\\\n",
    "                              sampler=train_sampler,\\\n",
    "                              batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(X_valid, valid_masks, Y_valid)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data,\\\n",
    "                                   sampler=validation_sampler,\\\n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs,\\\n",
    "          optimizer,\\\n",
    "          train_dataloader, valid_dataloader,\\\n",
    "          model_save_path,\\\n",
    "          train_loss_set=[], valid_loss_set = [],\\\n",
    "          lowest_eval_loss=None, start_epoch=0,\\\n",
    "          device=\"gpu\"\n",
    "          ):\n",
    "    \"\"\"\n",
    "    Train the model and save the model with the lowest validation loss\n",
    "    \"\"\"\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # trange is a tqdm wrapper around the normal python range\n",
    "    for i in trange(num_epochs, desc=\"Epoch\"):\n",
    "        # if continue training from saved model\n",
    "        actual_epoch = start_epoch + i\n",
    "\n",
    "        # Training\n",
    "\n",
    "        # Set our model to training mode (as opposed to evaluation mode)\n",
    "        model.train()\n",
    "\n",
    "        # Tracking variables\n",
    "        tr_loss = 0\n",
    "        num_train_samples = 0\n",
    "\n",
    "        # Train the data for one epoch\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            if step > 20000:\n",
    "                break\n",
    "            if step % 100 == 0:\n",
    "                print(step)\n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            # Clear out the gradients (by default they accumulate)\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            loss = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "            # store train loss\n",
    "            tr_loss += loss.item()\n",
    "            num_train_samples += b_labels.size(0)\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            # Update parameters and take a step using the computed gradient\n",
    "            optimizer.step()\n",
    "            #scheduler.step()\n",
    "\n",
    "    # Update tracking variables\n",
    "    epoch_train_loss = tr_loss/num_train_samples\n",
    "    train_loss_set.append(epoch_train_loss)\n",
    "    print(epoch_train_loss)\n",
    "    print(\"Train loss: {}\".format(epoch_train_loss))\n",
    "\n",
    "    # Validation\n",
    "\n",
    "    # Put model in evaluation mode to evaluate loss on the validation set\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss = 0\n",
    "    num_eval_samples = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for step, batch in enumerate(valid_dataloader):\n",
    "      # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Telling the model not to compute or store gradients,\n",
    "        # saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "        # Forward pass, calculate validation loss\n",
    "            loss = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        # store valid loss\n",
    "        eval_loss += loss.item()\n",
    "        num_eval_samples += b_labels.size(0)\n",
    "\n",
    "    epoch_eval_loss = eval_loss/num_eval_samples\n",
    "    valid_loss_set.append(epoch_eval_loss)\n",
    "\n",
    "    print(\"Valid loss: {}\".format(epoch_eval_loss))\n",
    "\n",
    "    if lowest_eval_loss == None:\n",
    "        lowest_eval_loss = epoch_eval_loss\n",
    "      # save model\n",
    "        save_model(model, model_save_path, actual_epoch,\\\n",
    "                 lowest_eval_loss, train_loss_set, valid_loss_set)\n",
    "    else:\n",
    "        if epoch_eval_loss < lowest_eval_loss:\n",
    "            lowest_eval_loss = epoch_eval_loss\n",
    "            # save model\n",
    "            save_model(model, model_save_path, actual_epoch,\\\n",
    "                       lowest_eval_loss, train_loss_set, valid_loss_set)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return model, train_loss_set, valid_loss_set\n",
    "\n",
    "\n",
    "def save_model(model, save_path, epochs, lowest_eval_loss, train_loss_hist, valid_loss_hist):\n",
    "    \"\"\"\n",
    "    Save the model to the path directory provided\n",
    "    \"\"\"\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    checkpoint = {'epochs': epochs, \\\n",
    "        'lowest_eval_loss': lowest_eval_loss,\\\n",
    "        'state_dict': model_to_save.state_dict(),\\\n",
    "        'train_loss_hist': train_loss_hist,\\\n",
    "        'valid_loss_hist': valid_loss_hist\n",
    "       }\n",
    "    cwd = os.getcwd()\n",
    "    save_dir = os.path.join(cwd,'Models')\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    torch.save(checkpoint, save_path)\n",
    "    print(\"Saving model at epoch {} with validation loss of {}\".format(epochs,\\\n",
    "                                                                     lowest_eval_loss))\n",
    "    return\n",
    "  \n",
    "def load_model(save_path):\n",
    "    \"\"\"\n",
    "    Load the model from the path directory provided\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(save_path)\n",
    "    model_state_dict = checkpoint['state_dict']\n",
    "    model = XLNetForMultiLabelSequenceClassification(num_labels=model_state_dict[\"classifier.weight\"].size()[0])\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "    epochs = checkpoint[\"epochs\"]\n",
    "    lowest_eval_loss = checkpoint[\"lowest_eval_loss\"]\n",
    "    train_loss_hist = checkpoint[\"train_loss_hist\"]\n",
    "    valid_loss_hist = checkpoint[\"valid_loss_hist\"]\n",
    "\n",
    "    return model, epochs, lowest_eval_loss, train_loss_hist, valid_loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = XLNetConfig()\n",
    "        \n",
    "class XLNetForMultiLabelSequenceClassification(torch.nn.Module):\n",
    "  \n",
    "  def __init__(self, num_labels=2):\n",
    "    super(XLNetForMultiLabelSequenceClassification, self).__init__()\n",
    "    self.num_labels = num_labels\n",
    "    self.xlnet = XLNetModel.from_pretrained('xlnet-base-cased', mem_len=1024)\n",
    "    self.classifier = torch.nn.Linear(768, num_labels)\n",
    "\n",
    "    torch.nn.init.xavier_normal_(self.classifier.weight)\n",
    "\n",
    "  def forward(self, input_ids, token_type_ids=None,\\\n",
    "              attention_mask=None, labels=None):\n",
    "    # last hidden layer\n",
    "    last_hidden_state = self.xlnet(input_ids=input_ids,\\\n",
    "                                   attention_mask=attention_mask,\\\n",
    "                                   token_type_ids=token_type_ids)\n",
    "    # pool the outputs into a mean vector\n",
    "    mean_last_hidden_state = self.pool_hidden_state(last_hidden_state)\n",
    "    logits = self.classifier(mean_last_hidden_state)\n",
    "        \n",
    "    if labels is not None:\n",
    "      loss_fct = BCEWithLogitsLoss()\n",
    "      loss = loss_fct(logits.view(-1, self.num_labels),\\\n",
    "                      labels.view(-1, self.num_labels))\n",
    "      return loss\n",
    "    else:\n",
    "      return logits\n",
    "    \n",
    "  def freeze_xlnet_decoder(self):\n",
    "    \"\"\"\n",
    "    Freeze XLNet weight parameters. They will not be updated during training.\n",
    "    \"\"\"\n",
    "    for param in self.xlnet.parameters():\n",
    "      param.requires_grad = False\n",
    "    \n",
    "  def unfreeze_xlnet_decoder(self):\n",
    "    \"\"\"\n",
    "    Unfreeze XLNet weight parameters. They will be updated during training.\n",
    "    \"\"\"\n",
    "    for param in self.xlnet.parameters():\n",
    "      param.requires_grad = True\n",
    "    \n",
    "  def pool_hidden_state(self, last_hidden_state):\n",
    "    \"\"\"\n",
    "    Pool the output vectors into a single mean vector \n",
    "    \"\"\"\n",
    "    last_hidden_state = last_hidden_state[0]\n",
    "    mean_last_hidden_state = torch.mean(last_hidden_state, 1)\n",
    "    return mean_last_hidden_state\n",
    "    \n",
    "model = XLNetForMultiLabelSequenceClassification(num_labels=len(Y_train[0]))\n",
    "#model = torch.nn.DataParallel(model)\n",
    "#model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01, correct_bias=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 1/1 [16:00<00:00, 960.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0011908560708451452\n",
      "Train loss: 0.0011908560708451452\n",
      "Valid loss: 8.085237623173394e-05\n",
      "Saving model at epoch 0 with validation loss of 8.085237623173394e-05\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs=1\n",
    "\n",
    "cwd = os.getcwd()\n",
    "model_save_path = output_model_file = os.path.join(cwd, \"Models/xlnet_fake.bin\")\n",
    "model, train_loss_set, valid_loss_set = train(model=model,\\\n",
    "                                              num_epochs=num_epochs,\\\n",
    "                                              optimizer=optimizer,\\\n",
    "                                              train_dataloader=train_dataloader,\\\n",
    "                                              valid_dataloader=validation_dataloader,\\\n",
    "                                              model_save_path=model_save_path,\\\n",
    "                                              device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model, df, num_labels, device=\"gpu\", batch_size=32):\n",
    "    num_iter = math.ceil(df.shape[0]/batch_size)\n",
    "\n",
    "    pred_probs = np.array([]).reshape(0, num_labels)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for i in range(num_iter):\n",
    "        df_subset = df.iloc[i*batch_size:(i+1)*batch_size,:]\n",
    "        X = df_subset[\"features\"].values.tolist()\n",
    "        masks = df_subset[\"masks\"].values.tolist()\n",
    "        X = torch.tensor(X)\n",
    "        masks = torch.tensor(masks, dtype=torch.long)\n",
    "        X = X.to(device)\n",
    "        masks = masks.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids=X, attention_mask=masks)\n",
    "            logits = logits.sigmoid().detach().cpu().numpy()\n",
    "            pred_probs = np.vstack([pred_probs, logits])\n",
    "\n",
    "    return pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.78410395e-04, 9.99891043e-01],\n",
       "       [7.14054622e-05, 9.99921918e-01],\n",
       "       [1.54350273e-04, 9.99890566e-01],\n",
       "       ...,\n",
       "       [1.20074728e-05, 9.99992132e-01],\n",
       "       [1.00000000e+00, 2.42506471e-12],\n",
       "       [1.00000000e+00, 4.48782027e-12]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = 2\n",
    "pred_probs = generate_predictions(model, test, num_labels, device=\"cuda\", batch_size=32)\n",
    "pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"real\"] = pred_probs[:,0]\n",
    "test[\"fake\"] = pred_probs[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"pred\"] = test[\"fake\"] > test[\"real\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     11798\n",
       "False    10651\n",
       "Name: pred, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"pred\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9993763642033052"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test[\"pred\"],test[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test[test['label']==1]['fake'].hist(figsize = (16,18),bins=200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test[test['label']==0]['fake'].hist(figsize = (16,18),bins=200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Now use scikit-learn's text classifier to train the model.\n",
    "vec = TfidfVectorizer(min_df = 3)\n",
    "model = MultinomialNB()\n",
    "\n",
    "train,test = train_test_split(df_full,test_size = 0.5,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(vec,model)\n",
    "clf = clf.fit(train['fulltext'], train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = test['label']\n",
    "test_X = test['fulltext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(test_X.astype('str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9368346028776338"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_y,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jorgeramosnews' 'ickes' 'icm' 'cdwyer0213' 'icna' 'icnc' 'cdf' 'qosi'\n",
      " 'cdata' 'cd' 'thump' 'ccbc' 'cbsthismorning' 'cbssports' 'thuggery'\n",
      " 'cbsnews' 'cbsla' 'cbs2' 'iconography' 'qnfpkcihzu']\n",
      "['zyuganov' 'rada' 'carme' 'carmakers' 'carlyle' 'radovan' 'carles'\n",
      " 'rafah' 'rabbu' 'rafale' 'caribou' 'cargoes' 'raggi' 'rai' 'caren'\n",
      " 'raila' 'railroads' 'rafik' 'rainsy' 'quynh']\n"
     ]
    }
   ],
   "source": [
    "neg_class_prob_sorted = model.feature_log_prob_[0, :].argsort()\n",
    "pos_class_prob_sorted = model.feature_log_prob_[1, :].argsort()\n",
    "\n",
    "print(np.take(vec.get_feature_names(), neg_class_prob_sorted[:20]))\n",
    "print(np.take(vec.get_feature_names(), pos_class_prob_sorted[:20]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
